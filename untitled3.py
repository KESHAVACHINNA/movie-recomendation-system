# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cSh0FH8a7FxJINTTIMml60eHryLNMd2O
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from wordcloud import WordCloud
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

df = pd.read_csv("movies.csv")

df.shape

df.head(2)

df.info()

required_columns = ["genres", "keywords", "overview", "title"]

df = df[required_columns]

df.head()

df.info()

df = df.dropna().reset_index(drop=True)

df.info()

df['combined'] = df['genres'] + ' ' + df['keywords'] + ' ' + df['overview']
df.head()

df.tail()

data = df[['title', 'combined']]
data.head()

data.shape

combined_text = " ".join(df['combined'])
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(combined_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Most Common Words in Movie content")
plt.show()

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

def preprocess_text(text):
    """
    This function preprocesses text by:
    1. Tokenizing the text into words.
    2. Removing stop words and punctuation.
    3. Converting words to lowercase.
    4. Joining the words back into a string.

    Args:
        text (str): The input text to preprocess.

    Returns:
        str: The preprocessed text.
    """
    tokens = word_tokenize(text)
    tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]
    preprocessed_text = " ".join(tokens)
    return preprocessed_text

data['cleaned_text'] = df['combined'].apply(preprocess_text)

data.head()

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
tfidf_matrix = tfidf_vectorizer.fit_transform(data['cleaned_text'])

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

print(cosine_sim[1])

def recommend_movies(movie_name, cosine_sim=cosine_sim, df=data, top_n=5):
    idx = df[df['title'].str.lower() == movie_name.lower()].index
    if len(idx) == 0:
        return "Movie not found in the dataset!"
    idx = idx[0]

    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n+1]

    movie_indices = [i[0] for i in sim_scores]

    return df[['title']].iloc[movie_indices]

data["title"]

row_index = df[df['title'] == "Batman v Superman: Dawn of Justice"].index
print(row_index)

movie_name = data["title"][9]
print(movie_name)

print(f"Recommendations for the Movie {movie_name}")
recommendations = recommend_movies(movie_name)
print(recommendations)

